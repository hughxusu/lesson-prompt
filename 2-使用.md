# 如何使用提示词

## 提示词的基本原则

1. 编写清晰而具体的提示指令。
   * 使用分隔符来清楚地表示输入的不同部分。
   * 要求一个结构化的输出。
   * 要求模型检查是否满足条件。
   * 提供少量示例。

2. 给模型思考的时间。
   * 指定完成任务所需的步骤。
   * 教导模型得出结论之前，先自己想办法解决问题。

### 编写清晰而具体的提示指令

示例一

```markdown
请告诉我如何写好大语言模型的提示词，以正反各5个例子，举例说明。
```

示例二

```markdown
请详述近两年来LLM模型的发展趋势，并引用数据来源，给出数据来源的链接。
请详述近三年来前端开发的发展趋势，并引用数据来源，给出数据来源的链接。
```

#### 使用分隔符来清楚地表示输入的不同部分

可以使用的分隔符是：```，""，<>，，<\tag>

示例一

````markdown
帮我翻译如下文字
```
Ever since the Turing Test was proposed in the 1950s, humans have explored the mastering of language intelligence by machine. Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable artificial intelligence (AI) algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. 
```
````

示例二

````markdown
帮我优化如下代码
```
export function isInvalidText(txt: unknown): boolean {
  if (typeof txt !== 'string') return true;
  if ((txt as string).match(/^\s+$/)) return true;
  if ((txt as string).match(/^[ ]+$/)) return true;
  if ((txt as string).match(/^[ ]*$/)) return true;
  if ((txt as string).match(/^\s*$/)) return true;
  return false;
}
```
````

#### 要求一个结构化的输出

示例一

```markdown
使用Scrapy开发一个网络爬虫，列出开发步骤。
```

示例二

```
详细介绍一下基于Python语言的服务端开发主流框架，并简述每个框架有何优缺点，以大纲的形式列出。
将上面的大纲以表格的形式展示。
将上面的大纲以Markdown的源码进行展示。
将上面的大纲转化为ppt形式。
```

#### 要求模型检查是否满足条件

```markdown
列举几个常用的前端框架，及主要的应用网站，以大纲的形式列出。
Umi框架是否为前端开发框架。
上述框架中是否包含手机端的开发框架。
```

#### 提供少量示例

````markdown
我是一名信息与计算科学专业的学生，我的专业课程包括如下课程
1. 高等数学
2. 概率与数理统计
3. 线性代数
4. 前端开发
5. python编程语言
6. mysql数据库
帮我写一份个人简介，格式如下
```
姓名：张三
性别：男
专业：XXX
个人介绍：
```
其中个人介绍部分不少于200字，用于大学生创业课题申报
````

### 给模型思考的时间

#### 指定完成任务所需的步骤

```markdown
给出近两年内中国大陆地区前端、服务端和数据分析师的岗位招聘数量和薪资情况。并列出上述三个岗位的求职要求和需要掌握的基本技能。我是一名信息与计算科学专业的学生。综合分析上述内容，我更适合上述哪个岗位。
```

将问题拆分成几个小问题。

````markdown
给出近两年内中国大陆地区前端、服务端和数据分析师的岗位招聘数量和薪资情况。
列出上述三个岗位的求职要求和需要掌握的基本技能。
我是一名信息与计算科学专业的学生，我的专业课程包括如下课程
```
1. 高等数学
2. 概率与数理统计
3. 线性代数
4. 前端开发
5. python编程语言
6. mysql数据库
```
根据专业课和岗位的匹配度，上述哪个岗位更适合我
````

#### 教导模型得出结论之前，先自己想办法解决问题

```markdown
我需要用python搭建Restful API的后台系统，如果基于flask框框架都需要使用哪些库，用大纲列出。
```

## 提示词的优化策略

1. 加入角色和背景信息。
2. 使用正确的语法和拼写，避免使用过于技术性或不常见的术语。
3. 让模型提问，以提供需要的背景信息。

### 加入角色和背景信息

```markdown
你是一名软件工程师，你熟悉html、css、JavaScript和TypeScript编程语言，react、umi和antd框架。
用antd写一个表单控件用于中国大陆地区身份证号的验证。
写一个手机号和密码登陆的页面。
```

### 使用正确的语法和拼写，避免使用过于技术性或不常见的术语

```markdown
SPA 是什么意思
```

### 让模型提问，以提供需要的背景信息

```markdown
如果我需要构建一个小说网站，你来帮我梳理需要构造网站的步骤。你可以向我提问你需要的背景信息或相关信息，以便你来规划网站所需的内容。
```

## Prompt的基本结构

角色+指令+任务数据+输出的格式要求

| 名称                     | 描述                              | 示例                                                       |
| ------------------------ | --------------------------------- | ---------------------------------------------------------- |
| 角色（非必须）           | 包括GPT模型的角色和你个人的角色。 | 1. 你是一名前端工程师<br>2. 我是一名信息与计算科学的本科生 |
| 指令词                   | 主要是需要完成的动作。            | 1. "简述"、"解释"、"翻译"、"总结"等。<br>2. 写一个ppt。    |
| 任务数据                 | 提供给模型的参考数据。            | 1.  翻译的文本。<br>2.  ppt的内容描述。                    |
| 输出的格式要求（非必须） | 限制输出形式                      | 1.  以大纲形式输出<br>2.  以表格形式输出                   |

[角色描述的指令网站](https://www.aishort.top/)

## 对话长度的控制

```markdown
你当前的版本可以记录多少个tokens
```

[测试tokens数量](https://platform.openai.com/tokenizer)

### 内容拓展

```markdown
写一篇基于ptyhon语言、flask框架和restful api风格的电商网站后台博文，2000字左右。
继续输出，再增加1000字的数量。
```

> [!warning]
>
> claude 工具可以接收的字符数量更多。

如果想写一篇长文可以先写大纲，再采用分布输出的方式。

### 长信息处理

第一步

```markdown
我将想你发送几条信息，如果文本中没有"<|endoftext|>"你只需要回复ok，如果收到后，则将前面的消息合并成一份中文摘要，以大纲的形式列出。
```

第二步

````markdown
第一部分
```
LANGUAGE is a prominent ability in human beings to express and communicate, which develops in early childhood and evolves over a lifetime [3, 4]. Machines, however, cannot naturally grasp the abilities of understand- ing and communicating in the form of human language, unless equipped with powerful artificial intelligence (AI) algorithms. It has been a longstanding research challenge to achieve this goal, to enable machines to read, write, and communicate like humans.
Technically, language modeling (LM) is one of the major approaches to advancing language intelligence of machines. In general, LM aims to model the generative likelihood of word sequences, so as to predict the probabilities of future (or missing) tokens. The research of LM has received extensive attention in the literature, which can be divided into four major development stages:
```

第二部分
```
Statistical language models (SLM). SLMs [6–9] are de- veloped based on statistical learning methods that rose in the 1990s. The basic idea is to build the word prediction model based on the Markov assumption, e.g., predicting the next word based on the most recent context. The SLMs with a fixed context length n are also called n-gram language models, e.g., bigram and trigram language models. SLMs have been widely applied to enhance task performance in information retrieval (IR) [10, 11] and natural language processing (NLP) [12–14]. However, they often suffer from the curse of dimensionality: it is difficult to accurately estimate high-order language models since an exponential number of transition probabilities need to be estimated. Thus, specially designed smoothing strategies such as back- off estimation [15] and Good–Turing estimation [16] have been introduced to alleviate the data sparsity problem.
```

第三部分
```
Neural language models (NLM). NLMs [1, 17, 18] charac- terize the probability of word sequences by neural networks, e.g., multi-layer perceptron (MLP) and recurrent neural net- works (RNNs). As a remarkable contribution, the work in [1] introduced the concept of distributed representation of words and built the word prediction function conditioned on the aggregated context features (i.e., the distributed word vectors). By extending the idea of learning effective features for text data, a general neural network approach was developed to build a unified, end-to-end solution for various NLP tasks [2]. Furthermore, word2vec [19, 20] was proposed to build a simplified shallow neural network for learning distributed word representations, which were demonstrated to be very effective across a variety of NLP tasks. These studies have initiated the use of language models for representation learning (beyond word sequence modeling), having an important impact on the field of NLP.
```

第四部分
```
Pre-trained language models (PLM). As an early at- tempt, ELMo [21] was proposed to capture context-aware word representations by first pre-training a bidirectional LSTM (biLSTM) network (instead of learning fixed word representations) and then fine-tuning the biLSTM network according to specific downstream tasks. Furthermore, based on the highly parallelizable Transformer architecture [22] with self-attention mechanisms, BERT [23] was proposed by pre-training bidirectional language models with specially designed pre-training tasks on large-scale unlabeled cor- pora. These pre-trained context-aware word representations are very effective as general-purpose semantic features, which have largely raised the performance bar of NLP tasks. This study has inspired a large number of follow-up work, which sets the “pre-training and fine-tuning” learning paradigm. Following this paradigm, a great number of stud- ies on PLMs have been developed, introducing either differ- ent architectures [24, 25] (e.g., GPT-2 [26] and BART [24]) or improved pre-training strategies [27–29]. In this paradigm, it often requires fine-tuning the PLM for adapting to different downstream tasks.
```

第五部分
```
Large language models (LLM). Researchers find that scaling PLM (e.g., scaling model size or data size) often leads to an improved model capacity on downstream tasks (i.e., following the scaling law [30]). A number of studies have explored the performance limit by training an ever larger PLM (e.g., the 175B-parameter GPT-3 and the 540B- parameter PaLM). Although scaling is mainly conducted in model size (with similar architectures and pre-training tasks), these large-sized PLMs display different behaviors from smaller PLMs (e.g., 330M-parameter BERT and 1.5B- parameter GPT-2) and show surprising abilities (called emer- gent abilities [31]) in solving a series of complex tasks. For example, GPT-3 can solve few-shot tasks through in-context learning, whereas GPT-2 cannot do well. Thus, the research community coins the term “large language models (LLM)”1 for these large-sized PLMs [32–35], which attract increasing research attention (See Figure 1). A remarkable application of LLMs is ChatGPT2 that adapts the LLMs from the GPT series for dialogue, which presents an amazing conversation ability with humans. We can observe a sharp increase of the arXiv papers that are related to LLMs after the release of ChatGPT in Figure 1.
```

第六部分
```
As discussed before, language model is not a new tech- nical concept specially for LLMs, but has evolved with the advance of artificial intelligence over the decades. Early lan- guage models mainly aim to model and generate text data, while latest language models (e.g., GPT-4) focus on complex task solving. From language modeling to task solving, it is an important leap in scientific thinking, which is the key to understand the development of language models in the re- search history. From the perspective of task solving, the four generations of language models have exhibited different lev- els of model capacities. In Figure 2, we describe the evolu- tion process of language models in terms of the task solving capacity. At first, statistical language models mainly assisted in some specific tasks (e.g., retrieval or speech tasks), in which the predicted or estimated probabilities can enhance the performance of task-specific approaches. Subsequently, neural language models focused on learning task-agnostic representations (e.g., features), aiming to reduce the efforts for human feature engineering. Furthermore, pre-trained language models learned context-aware representations that can be optimized according to downstream tasks. For the latest generation of language model, LLMs are enhanced by exploring the scaling effect on model capacity, which can be considered as general-purpose task solvers. To summarize, in the evolution process, the task scope that can be solved by language models have been greatly extended, and the task performance attained by language models have been significantly enhanced.
```
<|endoftext|>
````

## 角色设置

用户可以在"自定义 ChatGPT"设置背景信息和回复格式

背景信息

```markdown
角色：前端开发工程师
技能：html、css、JavaScript和TypeScript编程语言，react、umi和antd框架。
任务目标：网站开发
```

回复格式

```markdown
回复格式：
1. 代码优先
2. 说明简要
3. 说明尽量以大纲形式列出
语气：专业、轻松、略有幽默感
```

示例

```markdown
使用antd帮我写一个微信扫码登陆页
```

### 利用已知的prompt画像定制未知画像

````markdown
我是一个后端开发人员使用python和flask编程我需要自定义ChatGPT信息
请参考如下前端开发人员的自定义信息
```
你希望ChatGPT了解你哪些信息以便提供更好的回答？
角色：前端开发工程师
技能：html、css、JavaScript和TypeScript编程语言，react、umi和antd框架。
任务目标：网站开发
```
```
您希望ChatGPT如何回应您？
回复格式：
1. 代码优先
2. 说明简要
3. 说明尽量以大纲形式列出
语气：专业、轻松、略有幽默感
```
输出新的自定义信息
````

## 实用案例

案例一：撰写周报

```markdown
我是一名信息与计算科学的本科学生，我正在做毕业设计，我的毕业设计题目是《基于Python Flask的Restful Api接口开发》
这是毕业设计的第一周，本周主要工作如下：
1. 学习flask框架
2. 学习在flask框架操作数据库
3. 搭建项目环境
4. 学习路由的创建
根据上面的内容帮我写一篇300字左右的周报
```

